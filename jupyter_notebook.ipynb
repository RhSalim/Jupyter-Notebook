{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbcc9QS1tnBN"
   },
   "source": [
    "**DEVOIR 4 - Classification de textes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2Eeke4Z_EkW"
   },
   "source": [
    "1. Numéro du groupe: 22\n",
    "   Noms des membres: Salim Rholam\n",
    "   Numéros d'étudiants des membres: 300205835\n",
    "   Titre: \\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMUnCICdyBbs"
   },
   "source": [
    "**Datasets dérivés**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-so3pwbPKTDX"
   },
   "source": [
    "Ce notebook est un point de départ pour le devoir 4. Dans ce devoir, vous effectuerez une étude empirique de classification. Ce notebook vous aidera à créer des ensembles de données dérivés dans la section 2 du devoir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhFvS3q7Lu0R"
   },
   "outputs": [],
   "source": [
    "#let's start by installing spaCy\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aCWgl6PLKTDY"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX9WQWGSwU2D"
   },
   "source": [
    "Vous avez reçu une liste de datasets dans la description du devoir. Choisissez l'un des datasets, fournissez le lien ci-dessous et lisez ce dataset à l'aide de pandas. Vous devez fournir un lien vers votre propre répertoire Github même si vous utilisez une version réduite d'un ensemble de données du répertoire de votre TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSyg0jjC1jJa"
   },
   "source": [
    "Vous devrez ajouter une description de l'ensemble de données et votre justification des choix effectués pour obtenir les ensembles de données dérivés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1Xx4qMCLKTDb"
   },
   "outputs": [],
   "source": [
    "#Load the dataset you chose.\n",
    "# Make sure the Notebook can load your dataset, just like previous assignments.\n",
    "\n",
    "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_file_cnnnews.csv'\n",
    "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_drugsComTest_raw_fiveclasses.csv'\n",
    "url = 'https://raw.githubusercontent.com/RhSalim/dataset/22ba73653058f1df52ac156c6d0831349ead0034/reduced_file_cnnnews.csv'\n",
    "\n",
    "#provide the link to the raw version of dataset. You *need* to provide a link to *your own* github repository. DO NOT use the link that is provided as an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wg24OUV81Xgm"
   },
   "outputs": [],
   "source": [
    "print(url)\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQ5nSY1HKTDd"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3pycllPKTDd"
   },
   "source": [
    "Ici vous créez votre pipeline TAL. load() téléchargera le bon modèle pour l'analyse (English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wQtSi8XuKTDe"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccqFArDyKTDf"
   },
   "source": [
    "L'application du pipeline à chaque phrase crée un document dans lequel chaque mot est un objet Token.\n",
    "\n",
    "Doc: https://spacy.io/api/doc\n",
    "\n",
    "Token: https://spacy.io/api/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcaeMUL2KTDg"
   },
   "outputs": [],
   "source": [
    "#Apply nlp pipeline to the column that has your sentences (the text that will serve as input features).\n",
    "data['tokenized'] = data['Body'].apply(lambda text: nlp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i6ai1I8KTDg"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHRfZ2uEKTDh"
   },
   "source": [
    "Un token a plusieurs attributs, tel part-of-speech (pos_), lemma (lemma_), etc. Regardez la documentation pour voir l'ensemble des attributs.\n",
    "\n",
    "La fonction suivante est un exemple de la façon dont vous pouvez récupérer les parties du discours (POS)à partir d'une phrase. Nous renvoyons la lemmatisation car nous voulons uniquement le mot à l'infinitif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qw0a_2ySyUo2"
   },
   "outputs": [],
   "source": [
    "#create empty dataframes that will store your derived datasets\n",
    "\n",
    "derived_dataset1 = pd.DataFrame(columns = ['Class', 'pos'])\n",
    "derived_dataset2 = pd.DataFrame(columns = ['Class', 'pos-np'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Yeak1tAOKTDi"
   },
   "outputs": [],
   "source": [
    "def get_pos(sentence, wanted_pos): #wanted_pos refers to the desired pos tagging\n",
    "    verbs = []\n",
    "    for token in sentence:\n",
    "        if token.pos_ in wanted_pos:\n",
    "            verbs.append(token.lemma_) # lemma returns a number. lemma_ return a string\n",
    "    return ' '.join(verbs) # return value is as a string and not a list for countVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "147NRzwKKTDj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#As an example, we use the above function to fetch all the verbs. We store this information in our first derived dataset\n",
    "derived_dataset1['pos'] = data['tokenized'].apply(lambda sent : get_pos(sent, ['VERB']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_bUg_fVKTDk"
   },
   "outputs": [],
   "source": [
    "derived_dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuGv-NnfKTDj"
   },
   "outputs": [],
   "source": [
    "#Change this line to fetch your desired pos taggings for the second derived dataset\n",
    "derived_dataset2['pos-np'] = data['tokenized'].apply(lambda sent : get_pos(sent, ['ADJECTIVE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR7AdW0MfXO6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For Derived Dataset 2, you also need to include Named Entities\n",
    "#Below is just an example of obtaining such entities on a specific sentence, but you would do NER\n",
    "#on the dataset of your choice.\n",
    "#You can choose the types of entities (dates, organization, people) that you want,\n",
    "#and then in your derived dataset, just make sure you include these entities separated by spaces (as shown for verbs)\n",
    "#in a previous cell.\n",
    "\n",
    "sentence = \"apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pX4RgKhKTDk"
   },
   "source": [
    "Maintenant que vous disposez de vos ensembles de données dérivés, vous pouvez effectuer votre tâche de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GhniwHtzfQt"
   },
   "source": [
    "**Classification avec MLP et Régression logistique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming 'Class' column in the original dataset represents the target variable\n",
    "X = derived_dataset1['pos']  # Input features\n",
    "y = data['Class']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to numerical format using CountVectorizer or TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train the MLP classifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "mlp_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = mlp_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display additional classification metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your target variable in the 'Class' column\n",
    "y = data['Class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(derived_dataset1['pos'], y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model for derived dataset 1\n",
    "logistic_regression_model1 = LogisticRegression()\n",
    "logistic_regression_model1.fit(X_train.apply(lambda x: ' '.join(x)), y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions1 = logistic_regression_model1.predict(X_test.apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy1 = accuracy_score(y_test, predictions1)\n",
    "print(f\"Accuracy for derived dataset 1: {accuracy1}\")\n",
    "\n",
    "# Repeat the process for derived dataset 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(derived_dataset2['pos-np'], y, test_size=0.2, random_state=42)\n",
    "\n",
    "logistic_regression_model2 = LogisticRegression()\n",
    "logistic_regression_model2.fit(X_train.apply(lambda x: ' '.join(x)), y_train)\n",
    "\n",
    "predictions2 = logistic_regression_model2.predict(X_test.apply(lambda x: ' '.join(x)))\n",
    "\n",
    "accuracy2 = accuracy_score(y_test, predictions2)\n",
    "print(f\"Accuracy for derived dataset 2: {accuracy2}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
